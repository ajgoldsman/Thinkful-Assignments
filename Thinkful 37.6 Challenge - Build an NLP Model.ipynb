{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge: Build an NLP Model\n",
    "For this challenge, we chose a corpus of data from nltk or another source that includes categories to predict and create an analysis pipeline that includes the following steps:\n",
    "\n",
    "1. Data cleaning / processing / language parsing\n",
    "2. Create features using two different NLP methods: For example, BoW vs tf-idf.\n",
    "3. Use the features to fit supervised learning models for each feature set to predict the category outcomes.\n",
    "4. Assess your models using cross-validation and determine whether one model performed better.\n",
    "5. Pick one of the models and try to increase accuracy by at least 5 percentage points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import json\n",
    "import requests\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Shakespeare Works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data.\n",
    "caesar = gutenberg.raw('shakespeare-caesar.txt')\n",
    "hamlet = gutenberg.raw('shakespeare-hamlet.txt')\n",
    "macbeth = gutenberg.raw('shakespeare-macbeth.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove idiosyncratic Act (Actus) and Scene (Scena) headers\n",
    "caesar = re.sub(r'Actus .*', '', caesar)\n",
    "hamlet = re.sub(r'Actus .*', '', hamlet)\n",
    "macbeth = re.sub(r'Actus .*', '', macbeth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for standard text cleaning.\n",
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "    \n",
    "caesar_clean = text_cleaner(caesar)\n",
    "hamlet_clean = text_cleaner(hamlet)\n",
    "macbeth_clean = text_cleaner(macbeth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP via Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "\n",
    "caesar_doc = nlp(caesar_clean)\n",
    "hamlet_doc = nlp(hamlet_clean)\n",
    "macbeth_doc = nlp(macbeth_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2330\n",
      "3303\n",
      "2128\n"
     ]
    }
   ],
   "source": [
    "# Group into sentences.\n",
    "caesar_sents = [[sent, \"Caesar\"] for sent in caesar_doc.sents]\n",
    "hamlet_sents = [[sent, \"Hamlet\"] for sent in hamlet_doc.sents]\n",
    "macbeth_sents = [[sent, \"Macbeth\"] for sent in macbeth_doc.sents]\n",
    "\n",
    "print(len(caesar_sents))\n",
    "print(len(hamlet_sents))\n",
    "print(len(macbeth_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Enter, Flauius, ,, Murellus, ,, and, certaine...</td>\n",
       "      <td>Caesar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Flauius, .)</td>\n",
       "      <td>Caesar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Hence, :, home, you, idle, Creatures, ,, get,...</td>\n",
       "      <td>Caesar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Is, this, a, Holiday, ?)</td>\n",
       "      <td>Caesar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(What, ,, know, you, not, (, Being, Mechanical...</td>\n",
       "      <td>Caesar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0       1\n",
       "0  (Enter, Flauius, ,, Murellus, ,, and, certaine...  Caesar\n",
       "1                                       (Flauius, .)  Caesar\n",
       "2  (Hence, :, home, you, idle, Creatures, ,, get,...  Caesar\n",
       "3                          (Is, this, a, Holiday, ?)  Caesar\n",
       "4  (What, ,, know, you, not, (, Being, Mechanical...  Caesar"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Trim datasets so that document lengths are the same\n",
    "c_sents = caesar_sents[:len(macbeth_sents)]\n",
    "h_sents = hamlet_sents[:len(macbeth_sents)]\n",
    "\n",
    "# Combine the sentences from the two novels into one data frame.\n",
    "sentences = pd.DataFrame(c_sents + h_sents + macbeth_sents)\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create BoW Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to create a list of the 2000 most common words.\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "\n",
    "def bow_features(sentences, common_words):\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:,common_words] = 0\n",
    "    \n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        words = [token.lemma_ \n",
    "                for token in sentence\n",
    "                if (\n",
    "                    not token.is_punct\n",
    "                    and not token.is_stop\n",
    "                    and token.lemma_ in common_words\n",
    "                )]\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        if i%500 == 0:\n",
    "            print('Processing row {}'.format(i))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reset docs with new set of sentences\n",
    "new_caesar_doc = ' '.join([(str(sent)) for sent in pd.DataFrame(c_sents)[0]])\n",
    "new_hamlet_doc = ' '.join([(str(sent)) for sent in pd.DataFrame(h_sents)[0]])\n",
    "\n",
    "# Set up the bags.\n",
    "caesarwords = bag_of_words(nlp(new_caesar_doc))\n",
    "hamletwords = bag_of_words(nlp(new_hamlet_doc))\n",
    "macbethwords = bag_of_words(macbeth_doc)\n",
    "\n",
    "# Combine bags to create a set of unique words.\n",
    "common_words = set(caesarwords + hamletwords + macbethwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 500\n",
      "Processing row 1000\n",
      "Processing row 1500\n",
      "Processing row 2000\n",
      "Processing row 2500\n",
      "Processing row 3000\n",
      "Processing row 3500\n",
      "Processing row 4000\n",
      "Processing row 4500\n",
      "Processing row 5000\n",
      "Processing row 5500\n",
      "Processing row 6000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>broad</th>\n",
       "      <th>orewhelm</th>\n",
       "      <th>sightlesse</th>\n",
       "      <th>chafe</th>\n",
       "      <th>Bear't</th>\n",
       "      <th>bonfire</th>\n",
       "      <th>Implements</th>\n",
       "      <th>bray</th>\n",
       "      <th>plunge</th>\n",
       "      <th>Byrnan</th>\n",
       "      <th>...</th>\n",
       "      <th>dis</th>\n",
       "      <th>body</th>\n",
       "      <th>smooth</th>\n",
       "      <th>grant</th>\n",
       "      <th>corporall</th>\n",
       "      <th>Holyday</th>\n",
       "      <th>friend</th>\n",
       "      <th>conceiue</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Enter, Flauius, ,, Murellus, ,, and, certaine...</td>\n",
       "      <td>Caesar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Flauius, .)</td>\n",
       "      <td>Caesar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Hence, :, home, you, idle, Creatures, ,, get,...</td>\n",
       "      <td>Caesar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Is, this, a, Holiday, ?)</td>\n",
       "      <td>Caesar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(What, ,, know, you, not, (, Being, Mechanical...</td>\n",
       "      <td>Caesar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4318 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  broad orewhelm sightlesse chafe Bear't bonfire Implements bray plunge  \\\n",
       "0     0        0          0     0      0       0          0    0      0   \n",
       "1     0        0          0     0      0       0          0    0      0   \n",
       "2     0        0          0     0      0       0          0    0      0   \n",
       "3     0        0          0     0      0       0          0    0      0   \n",
       "4     0        0          0     0      0       0          0    0      0   \n",
       "\n",
       "  Byrnan  ... dis body smooth grant corporall Holyday friend conceiue  \\\n",
       "0      0  ...   0    0      0     0         0       0      0        0   \n",
       "1      0  ...   0    0      0     0         0       0      0        0   \n",
       "2      0  ...   0    0      0     0         0       0      0        0   \n",
       "3      0  ...   0    0      0     0         0       0      0        0   \n",
       "4      0  ...   0    0      0     0         0       0      0        0   \n",
       "\n",
       "                                       text_sentence text_source  \n",
       "0  (Enter, Flauius, ,, Murellus, ,, and, certaine...      Caesar  \n",
       "1                                       (Flauius, .)      Caesar  \n",
       "2  (Hence, :, home, you, idle, Creatures, ,, get,...      Caesar  \n",
       "3                          (Is, this, a, Holiday, ?)      Caesar  \n",
       "4  (What, ,, know, you, not, (, Being, Mechanical...      Caesar  \n",
       "\n",
       "[5 rows x 4318 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create features\n",
    "word_counts = bow_features(sentences, common_words)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6384, 4318)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create tf-idf Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_list = [str(sent) for sent in sentences[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 3161\n",
      "Original sentence: Be Lyon metled, proud, and take no care:\n",
      "Tf_idf vector: {'metled': 0.49101402450721193, 'proud': 0.49101402450721193, 'care': 0.4429901765213423, 'Lyon': 0.45172725607865366, 'Be': 0.342801174291604}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test = train_test_split(sents_list, test_size=0.4, random_state=0)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, # drop words that occur in more than half the sentences\n",
    "                             min_df=2, # only use words that appear at least twice\n",
    "                             stop_words='english', \n",
    "                             lowercase=True, #Shakespeare tends to capitalize words for emphasis\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer sentences and shorter sentences get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )\n",
    "\n",
    "\n",
    "#Applying the vectorizer\n",
    "sents_tfidf=vectorizer.fit_transform(sents_list)\n",
    "print(\"Number of features: %d\" % sents_tfidf.get_shape()[1])\n",
    "\n",
    "#splitting into training and test sets\n",
    "X_train_tfidf, X_test_tfidf= train_test_split(sents_tfidf, test_size=0.4, random_state=0)\n",
    "\n",
    "\n",
    "#Reshapes the vectorizer output into something people can read\n",
    "X_train_tfidf_csr = X_train_tfidf.tocsr()\n",
    "\n",
    "#number of sentences\n",
    "n = X_train_tfidf_csr.shape[0]\n",
    "#A list of dictionaries, one per sentence\n",
    "tfidf_bysent = [{} for _ in range(0,n)]\n",
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "#for each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_bysent[i][terms[j]] = X_train_tfidf_csr[i, j]\n",
    "\n",
    "#Keep in mind that the log base 2 of 1 is 0, so a tf-idf score of 0 indicates that the word was present once in that sentence.\n",
    "print('Original sentence:', X_train[5])\n",
    "print('Tf_idf vector:', tfidf_bysent[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning: SVD Data Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by all components: 38.150758120041104\n",
      "Component 0:\n",
      "Ham.    0.966633\n",
      "Ham.    0.966633\n",
      "Ham.    0.966633\n",
      "Ham.    0.966633\n",
      "Ham.    0.966633\n",
      "Ham.    0.966633\n",
      "Ham.    0.966633\n",
      "Ham.    0.966633\n",
      "Ham.    0.966633\n",
      "Ham.    0.966633\n",
      "Name: 0, dtype: float64\n",
      "Component 1:\n",
      "my Lord, which was reported Macb.    0.862063\n",
      "Macb.                                0.842634\n",
      "Macb.                                0.842634\n",
      "Macb.                                0.842634\n",
      "Macb.                                0.842634\n",
      "Macb.                                0.842634\n",
      "' Building Macb.                     0.842634\n",
      "Macb.                                0.842634\n",
      "Macb.                                0.842634\n",
      "Macb.                                0.842634\n",
      "Name: 1, dtype: float64\n",
      "Component 2:\n",
      "Enter.    0.949122\n",
      "Enter.    0.949122\n",
      "Enter.    0.949122\n",
      "Enter.    0.949122\n",
      "Enter.    0.949122\n",
      "Enter.    0.949122\n",
      "Enter.    0.949122\n",
      "Enter.    0.949122\n",
      "Enter.    0.949122\n",
      "Enter.    0.949122\n",
      "Name: 2, dtype: float64\n",
      "Component 3:\n",
      "Exeunt.          0.999211\n",
      "Exeunt.          0.999211\n",
      "Exeunt.          0.999211\n",
      "Exeunt.          0.999211\n",
      "Exeunt.          0.999211\n",
      "Exeunt.          0.999211\n",
      "Exeunt.          0.999211\n",
      "Exeunt.          0.999211\n",
      "Exeunt.          0.999211\n",
      "Exeunt Omnes.    0.999211\n",
      "Name: 3, dtype: float64\n",
      "Component 4:\n",
      "What speech, my Lord?               0.699567\n",
      "What is my Lord?                    0.695901\n",
      "What do you read my Lord?           0.695098\n",
      "What sights, my Lord?               0.694836\n",
      "What meanes this, my Lord?          0.694258\n",
      "What newes, my Lord?                0.686030\n",
      "What, frighted with false fire      0.596712\n",
      ": What's done, cannot be vndone.    0.593350\n",
      "What's he                           0.592567\n",
      "What with Wormes, and Flyes?        0.592567\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "#Our SVD data reducer.  We are going to reduce the feature space from 3161 to 130.\n",
    "svd= TruncatedSVD(130)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)\n",
    "\n",
    "#Looking at what sorts of paragraphs our solution considers similar, for the first five identified topics\n",
    "sents_by_component=pd.DataFrame(X_train_lsa,index=X_train)\n",
    "for i in range(5):\n",
    "    print('Component {}:'.format(i))\n",
    "    print(sents_by_component.loc[:,i].sort_values(ascending=False)[0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key:\n",
      "0 Not so my Lord, I am too much i'th' Sun Queen.\n",
      "1 hew him as a Carkasse fit for Hounds: And let our Hearts, as subtle Masters do, Stirre vp their Seruants to an acte of Rage, And after seeme to chide 'em.\n",
      "2 We ore-wrought on the way: of these we told him, And there did seeme in him a kinde of ioy\n",
      "3 Another generall shout?\n",
      "4 Giue me that man, That is not Passions Slaue, and I will weare him In my hearts Core.\n",
      "5 Be Lyon metled, proud, and take no care:\n",
      "6 At home, my Lord Ham.\n",
      "7 Enter Portia.\n",
      "8 All haile Macbeth, haile to thee Thane of Glamis 2.\n",
      "9 Nay, good my Lord, this courtesie is not of the right breed.\n"
     ]
    }
   ],
   "source": [
    "# Compute document similarity using LSA components\n",
    "similarity = np.asarray(np.asmatrix(X_train_lsa) * np.asmatrix(X_train_lsa).T)\n",
    "#Only taking the first 10 sentences\n",
    "sim_matrix=pd.DataFrame(similarity,index=X_train).iloc[0:10,0:10]\n",
    "#Making a plot\n",
    "ax = sns.heatmap(sim_matrix,yticklabels=range(10))\n",
    "plt.show()\n",
    "\n",
    "#Generating a key for the plot.\n",
    "print('Key:')\n",
    "for i in range(10):\n",
    "    print(i,sim_matrix.index[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning: Random Forest, Logistic Regression, and Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9370757180156658\n",
      "\n",
      "Test set score: 0.6468285043069695\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "Y = word_counts['text_source']\n",
    "X = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y,\n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=0)\n",
    "train = rfc.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3830, 4316) (3830,)\n",
      "Training set score: 0.891644908616188\n",
      "\n",
      "Test set score: 0.677760375880971\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "train = lr.fit(X_train, y_train)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.6702349869451697\n",
      "\n",
      "Test set score: 0.6221613155833986\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf = GradientBoostingClassifier()\n",
    "train = clf.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', clf.score(X_train, y_train))\n",
    "print('\\nTest set score:', clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the issues with this version of the dataset is that, since we're dealing with Shakespeare plays, the names of the characters appear on new lines, which leads to skewed components with the SVD Reduction. By separating out chunks of text (or paragraphs) from each play, instead of sentences, we can possibly bypass that component grouping issue. Additionally, let's try including other features of each section of the text to see if we can improve the accuracy scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning and Paragraph Separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def par_clean(text):\n",
    "    #Remove idiosyncratic Act and Scene Labels, as well as enter and exit directions.\n",
    "    text = re.sub(r'Exeunt.*', '', text)\n",
    "    text = re.sub(r'Enter.*', '', text)\n",
    "    text = re.sub(r'Actus .*', '', text)\n",
    "    text = re.sub(r'Scena .*', '', text)\n",
    "    text = re.sub(r'Exit .*', '', text)\n",
    "    \n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    \n",
    "    paras_list = [re.sub(r'^\\w+\\.', '', item.strip()).strip() for item in re.split(r\"\\n\", text)]\n",
    "\n",
    "    new_paras_list = []\n",
    "    current_section = ''\n",
    "    for i in range(len(paras_list)):\n",
    "        if paras_list[i] == '':\n",
    "            new_paras_list.append(current_section)\n",
    "            current_section = ''\n",
    "        else:\n",
    "            current_section = current_section + ' ' + paras_list[i]\n",
    "\n",
    "    return [item.strip() for item in new_paras_list if item != '']        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "697\n",
      "899\n",
      "590\n"
     ]
    }
   ],
   "source": [
    "# Group into paragraphs.\n",
    "caesar_paras = [[nlp(par), \"Caesar\"] for par in par_clean(caesar)]\n",
    "hamlet_paras = [[nlp(par), \"Hamlet\"] for par in par_clean(hamlet)]\n",
    "macbeth_paras = [[nlp(par), \"Macbeth\"] for par in par_clean(macbeth)]\n",
    "\n",
    "print(len(caesar_paras))\n",
    "print(len(hamlet_paras))\n",
    "print(len(macbeth_paras))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Hence, :, home, you, idle, Creatures, ,, get,...</td>\n",
       "      <td>Caesar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Where, is, thy, Leather, Apron, ,, and, thy, ...</td>\n",
       "      <td>Caesar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(But, what, Trade, art, thou, ?, Answer, me, d...</td>\n",
       "      <td>Caesar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(A, Trade, Sir, ,, that, I, hope, I, may, vse,...</td>\n",
       "      <td>Caesar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(What, Trade, thou, knaue, ?, Thou, naughty, k...</td>\n",
       "      <td>Caesar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0       1\n",
       "0  (Hence, :, home, you, idle, Creatures, ,, get,...  Caesar\n",
       "1  (Where, is, thy, Leather, Apron, ,, and, thy, ...  Caesar\n",
       "2  (But, what, Trade, art, thou, ?, Answer, me, d...  Caesar\n",
       "3  (A, Trade, Sir, ,, that, I, hope, I, may, vse,...  Caesar\n",
       "4  (What, Trade, thou, knaue, ?, Thou, naughty, k...  Caesar"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Trim datasets so that document lengths are the same\n",
    "c_paras = caesar_paras[:len(macbeth_paras)]\n",
    "h_paras = hamlet_paras[:len(macbeth_paras)]\n",
    "\n",
    "# Combine the sentences from the two novels into one data frame.\n",
    "paragraphs = pd.DataFrame(c_paras + h_paras + macbeth_paras)\n",
    "paragraphs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reset docs with new set of sentences\n",
    "paras_caesar_doc = ' '.join([(str(par)) for par in pd.DataFrame(c_paras)[0]])\n",
    "paras_hamlet_doc = ' '.join([(str(par)) for par in pd.DataFrame(h_paras)[0]])\n",
    "paras_macbeth_doc = ' '.join([(str(par)) for par in pd.DataFrame(macbeth_paras)[0]])\n",
    "\n",
    "# Set up the bags.\n",
    "caesarwords = bag_of_words(nlp(paras_caesar_doc))\n",
    "hamletwords = bag_of_words(nlp(paras_hamlet_doc))\n",
    "macbethwords = bag_of_words(nlp(paras_macbeth_doc))\n",
    "\n",
    "# Combine bags to create a set of unique words.\n",
    "new_common_words = set(caesarwords + hamletwords + macbethwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define New BoW Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def par_bow_features(paragraphs, common_words):\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_paragraph'] = paragraphs[0]\n",
    "    df['text_source'] = paragraphs[1]\n",
    "    df.loc[:,common_words] = 0\n",
    "    \n",
    "    for i, paragraph in enumerate(df['text_paragraph']):\n",
    "        words = [token.lemma_ \n",
    "                for token in paragraph\n",
    "                if (\n",
    "                    not token.is_punct\n",
    "                    and not token.is_stop\n",
    "                    and token.lemma_ in common_words\n",
    "                )]\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        if i%500 == 0:\n",
    "            print('Processing row {}'.format(i))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>broad</th>\n",
       "      <th>orewhelm</th>\n",
       "      <th>sightlesse</th>\n",
       "      <th>chafe</th>\n",
       "      <th>Bear't</th>\n",
       "      <th>bonfire</th>\n",
       "      <th>Implements</th>\n",
       "      <th>bray</th>\n",
       "      <th>plunge</th>\n",
       "      <th>Byrnan</th>\n",
       "      <th>...</th>\n",
       "      <th>dis</th>\n",
       "      <th>body</th>\n",
       "      <th>smooth</th>\n",
       "      <th>grant</th>\n",
       "      <th>corporall</th>\n",
       "      <th>Holyday</th>\n",
       "      <th>friend</th>\n",
       "      <th>conceiue</th>\n",
       "      <th>text_paragraph</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Hence, :, home, you, idle, Creatures, ,, get,...</td>\n",
       "      <td>Caesar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Where, is, thy, Leather, Apron, ,, and, thy, ...</td>\n",
       "      <td>Caesar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(But, what, Trade, art, thou, ?, Answer, me, d...</td>\n",
       "      <td>Caesar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(A, Trade, Sir, ,, that, I, hope, I, may, vse,...</td>\n",
       "      <td>Caesar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(What, Trade, thou, knaue, ?, Thou, naughty, k...</td>\n",
       "      <td>Caesar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4303 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  broad orewhelm sightlesse chafe Bear't bonfire Implements bray plunge  \\\n",
       "0     0        0          0     0      0       0          0    0      0   \n",
       "1     0        0          0     0      0       0          0    0      0   \n",
       "2     0        0          0     0      0       0          0    0      0   \n",
       "3     0        0          0     0      0       0          0    0      0   \n",
       "4     0        0          0     0      0       0          0    0      0   \n",
       "\n",
       "  Byrnan  ... dis body smooth grant corporall Holyday friend conceiue  \\\n",
       "0      0  ...   0    0      0     0         0       0      0        0   \n",
       "1      0  ...   0    0      0     0         0       0      0        0   \n",
       "2      0  ...   0    0      0     0         0       0      0        0   \n",
       "3      0  ...   0    0      0     0         0       0      0        0   \n",
       "4      0  ...   0    0      0     0         0       0      0        0   \n",
       "\n",
       "                                      text_paragraph text_source  \n",
       "0  (Hence, :, home, you, idle, Creatures, ,, get,...      Caesar  \n",
       "1  (Where, is, thy, Leather, Apron, ,, and, thy, ...      Caesar  \n",
       "2  (But, what, Trade, art, thou, ?, Answer, me, d...      Caesar  \n",
       "3  (A, Trade, Sir, ,, that, I, hope, I, may, vse,...      Caesar  \n",
       "4  (What, Trade, thou, knaue, ?, Thou, naughty, k...      Caesar  \n",
       "\n",
       "[5 rows x 4303 columns]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create features\n",
    "new_word_counts = par_bow_features(paragraphs, new_common_words)\n",
    "new_word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1732, 4303)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_word_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create New tf-idf Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "paras_list = [str(par) for par in paragraphs[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 1205\n",
      "Original sentence: Could Beautie my Lord, haue better Comerce then your Honestie? I trulie: for the power of Beautie, will sooner transforme Honestie from what is, to a Bawd, then the force of Honestie can translate Beautie into his likenesse. This was sometime a Paradox, but now the time giues it I did loue you once\n",
      "Tf_idf vector: {'sooner': 0.44059191427500544, 'force': 0.4189442453330255, 'power': 0.3514016687825042, 'better': 0.3191832077412201, 'giues': 0.3743491225867749, 'did': 0.2399918646941736, 'loue': 0.26221142329000285, 'lord': 0.1983084226321303, 'time': 0.256814726629944, 'haue': 0.18230525484808208}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test = train_test_split(paras_list, test_size=0.4, random_state=0)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.7, # drop words that occur in more than half the sentences\n",
    "                             min_df=4, # only use words that appear at least twice\n",
    "                             stop_words='english', \n",
    "                             lowercase=True, #Shakespeare tends to capitalize words for emphasis\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer sentences and shorter sentences get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )\n",
    "\n",
    "\n",
    "#Applying the vectorizer\n",
    "paras_tfidf=vectorizer.fit_transform(paras_list)\n",
    "print(\"Number of features: %d\" % paras_tfidf.get_shape()[1])\n",
    "\n",
    "#splitting into training and test sets\n",
    "X_train_tfidf, X_test_tfidf= train_test_split(paras_tfidf, test_size=0.4, random_state=0)\n",
    "\n",
    "\n",
    "#Reshapes the vectorizer output into something people can read\n",
    "X_train_tfidf_csr = X_train_tfidf.tocsr()\n",
    "\n",
    "#number of paragraphs\n",
    "n = X_train_tfidf_csr.shape[0]\n",
    "#A list of dictionaries, one per paragraph\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "#for each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_train_tfidf_csr[i, j]\n",
    "\n",
    "#Keep in mind that the log base 2 of 1 is 0, so a tf-idf score of 0 indicates that the word was present once in that sentence.\n",
    "print('Original sentence:', X_train[5])\n",
    "print('Tf_idf vector:', tfidf_bypara[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning - New Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by all components: 50.295730439107814\n",
      "Component 0:\n",
      "You are merrie, my Lord? Who I? I my Lord    0.902525\n",
      "My Lord, I will not                          0.902525\n",
      "I will my Lord                               0.902525\n",
      "Not I my Lord                                0.902525\n",
      "Propose the Oath my Lord                     0.902525\n",
      "I will my Lord.                              0.902525\n",
      "& Mar. within. My Lord, my Lord.             0.902525\n",
      "I will, my Lord.                             0.902525\n",
      "Nor I, my Lord                               0.902525\n",
      "My Lord that would dishonour him             0.902525\n",
      "Name: 0, dtype: float64\n",
      "\n",
      "\n",
      "Component 1:\n",
      "You are merrie, my Lord? Who I? I my Lord    0.380909\n",
      "My Lord, I will not                          0.380909\n",
      "I will, my Lord.                             0.380909\n",
      "What is't my Lord? we will                   0.380909\n",
      "My Lord                                      0.380909\n",
      "I will my Lord                               0.380909\n",
      "Nor I, my Lord                               0.380909\n",
      "I will my Lord.                              0.380909\n",
      "Not I my Lord                                0.380909\n",
      "Infaith my Lord, not I                       0.380909\n",
      "Name: 1, dtype: float64\n",
      "\n",
      "\n",
      "Component 2:\n",
      "The Games are done, And Caesar is returning                                                                                                                                                                                                                                                                                                                0.612349\n",
      "Caesar                                                                                                                                                                                                                                                                                                                                                     0.612349\n",
      "Shall no man else be toucht, but onely Caesar? Decius well vrg'd: I thinke it is not meet, Marke Antony, so well belou'd of Caesar, Should out-liue Caesar, we shall finde of him A shrew'd Contriuer. And you know, his meanes If he improue them, may well stretch so farre As to annoy vs all: which to preuent, Let Antony and Caesar fall together    0.566924\n",
      "I shall remember, When Caesar sayes, Do this; it is perform'd                                                                                                                                                                                                                                                                                              0.561825\n",
      "O Caesar, reade mine first: for mine's a suite That touches Caesar neerer. Read it great Caesar                                                                                                                                                                                                                                                            0.548775\n",
      "Delay not Caesar, read it instantly                                                                                                                                                                                                                                                                                                                        0.546677\n",
      "Caes The Gods do this in shame of Cowardice: Caesar should be a Beast without a heart If he should stay at home to day for feare: No Caesar shall not; Danger knowes full well That Caesar is more dangerous then he. We heare two Lyons litter'd in one day, And I the elder and more terrible, And Caesar shall go foorth                                0.534652\n",
      "Caes Et Tu Brute? - Then fall Caesar.                                                                                                                                                                                                                                                                                                                      0.493371\n",
      "So to most Noble Caesar                                                                                                                                                                                                                                                                                                                                    0.480231\n",
      "When Caesar liu'd, he durst not thus haue mou'd me                                                                                                                                                                                                                                                                                                         0.436816\n",
      "Name: 2, dtype: float64\n",
      "\n",
      "\n",
      "Component 3:\n",
      "Caesar                                                                                                                                                                                           0.531710\n",
      "The Games are done, And Caesar is returning                                                                                                                                                      0.531710\n",
      "Thou hast some suite to Caesar, hast thou not? That I haue Lady, if it will please Caesar To be so good to Caesar, as to heare me: I shall beseech him to befriend himselfe                      0.531028\n",
      "Good morrow Caesar                                                                                                                                                                               0.521652\n",
      "O Caesar, reade mine first: for mine's a suite That touches Caesar neerer. Read it great Caesar                                                                                                  0.483761\n",
      "Delay not Caesar, read it instantly                                                                                                                                                              0.476103\n",
      "Caes Are we all ready? What is now amisse, That Caesar and his Senate must redresse? Most high, most mighty, and most puisant Caesar Metellus Cymber throwes before thy Seate An humble heart    0.452157\n",
      "So to most Noble Caesar                                                                                                                                                                          0.445559\n",
      "What ere thou art, for thy good caution, thanks Thou hast harp'd my feare aright. But one word more                                                                                              0.443039\n",
      "Caes Et Tu Brute? - Then fall Caesar.                                                                                                                                                            0.427743\n",
      "Name: 3, dtype: float64\n",
      "\n",
      "\n",
      "Component 4:\n",
      "Is he alone? No, Sir, there are moe with him                                                               0.811927\n",
      "Sir, a whole History                                                                                       0.811927\n",
      "Sir, I cannot                                                                                              0.811927\n",
      "Sir I lacke Aduancement                                                                                    0.788258\n",
      "Well, say sir                                                                                              0.779429\n",
      "Geese Villaine? Souldiers Sir                                                                              0.758170\n",
      "A Trade Sir, that I hope I may vse, with a safe Conscience, which is indeed Sir, a Mender of bad soules    0.709988\n",
      "The King, sir                                                                                              0.663003\n",
      "Sir, Amen                                                                                                  0.636776\n",
      "Most Royall Sir Fleans is scap'd                                                                           0.592588\n",
      "Name: 4, dtype: float64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "#Our SVD data reducer.  We are going to reduce the feature space from 1205 to 130.\n",
    "svd= TruncatedSVD(130)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)\n",
    "\n",
    "#Looking at what sorts of paragraphs our solution considers similar, for the first five identified topics\n",
    "paras_by_component=pd.DataFrame(X_train_lsa,index=X_train)\n",
    "for i in range(5):\n",
    "    print('Component {}:'.format(i))\n",
    "    print(paras_by_component.loc[:,i].sort_values(ascending=False)[0:10])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key:\n",
      "0 See where their basest mettle be not mou'd, They vanish tongue-tyed in their guiltinesse: Go you downe that way towards the Capitoll, This way will I: Disrobe the Images, If you do finde them deckt with Ceremonies\n",
      "1 If it assume my noble Fathers person, Ile speake to it, though Hell it selfe should gape And bid me hold my peace. I pray you all, If you haue hitherto conceald this sight; Let it bee treble in your silence still: And whatsoeuer els shall hap to night, Giue it an vnderstanding but no tongue; I will requite your loues; so fare ye well: Vpon the Platforme twixt eleuen and twelue, Ile visit you\n",
      "2 My young remembrance cannot paralell A fellow to it.\n",
      "3 Drinkes\n",
      "4 And what did you enact? I did enact Iulius Caesar, I was kill'd i'th' Capitol: Brutus kill'd me\n",
      "5 Could Beautie my Lord, haue better Comerce then your Honestie? I trulie: for the power of Beautie, will sooner transforme Honestie from what is, to a Bawd, then the force of Honestie can translate Beautie into his likenesse. This was sometime a Paradox, but now the time giues it I did loue you once\n",
      "6 Shew his Eyes, and greeue his Hart, Come like shadowes, so depart.\n",
      "7 Was the Crowne offer'd him thrice? I marry was't, and hee put it by thrice, euerie time gentler then other; and at euery putting by, mine honest Neighbors showted\n",
      "8 Go too, go too: You haue knowne what you should not\n",
      "9 The weyward Sisters, hand in hand, Posters of the Sea and Land, Thus doe goe, about, about, Thrice to thine, and thrice to mine, And thrice againe, to make vp nine. Peace, the Charme's wound vp.\n"
     ]
    }
   ],
   "source": [
    "# Compute document similarity using LSA components\n",
    "similarity = np.asarray(np.asmatrix(X_train_lsa) * np.asmatrix(X_train_lsa).T)\n",
    "#Only taking the first 10 sentences\n",
    "sim_matrix=pd.DataFrame(similarity,index=X_train).iloc[0:10,0:10]\n",
    "#Making a plot\n",
    "ax = sns.heatmap(sim_matrix,yticklabels=range(10))\n",
    "plt.show()\n",
    "\n",
    "#Generating a key for the plot.\n",
    "print('Key:')\n",
    "for i in range(10):\n",
    "    print(i,sim_matrix.index[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning - New Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.983638113570741\n",
      "\n",
      "Test set score: 0.6118326118326118\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "Y = new_word_counts['text_source']\n",
    "X = np.array(new_word_counts.drop(['text_paragraph','text_source'], 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y,\n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=0)\n",
    "train = rfc.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1039, 4301) (1039,)\n",
      "Training set score: 0.9547641963426372\n",
      "\n",
      "Test set score: 0.6825396825396826\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "train = lr.fit(X_train, y_train)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.7757459095283927\n",
      "\n",
      "Test set score: 0.6421356421356421\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf = GradientBoostingClassifier()\n",
    "train = clf.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', clf.score(X_train, y_train))\n",
    "print('\\nTest set score:', clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning - w/ Added Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_word_counts['sent_length'] = new_word_counts['text_paragraph'].map(lambda x: len(x)) \n",
    "new_word_counts['num_verbs'] = [len([token.text.strip() for token in par if token.pos_=='VERB']) for par in paragraphs[0]]\n",
    "new_word_counts['num_nouns'] = [len([token.text.strip() for token in par if token.pos_=='NOUN']) for par in paragraphs[0]]\n",
    "new_word_counts['num_adverbs'] = [len([token.text.strip() for token in par if token.pos_=='ADV']) for par in paragraphs[0]]\n",
    "new_word_counts['num_total_punct'] = [len([token.text.strip() for token in par if token.pos_ == 'PUNCT']) for par in paragraphs[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9913378248315688\n",
      "\n",
      "Test set score: 0.6305916305916306\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "Y2 = new_word_counts['text_source']\n",
    "X2 = np.array(new_word_counts.drop(['text_paragraph','text_source'], 1))\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, \n",
    "                                                    Y2,\n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=0)\n",
    "train = rfc.fit(X_train2, y_train2)\n",
    "\n",
    "print('Training set score:', rfc.score(X_train2, y_train2))\n",
    "print('\\nTest set score:', rfc.score(X_test2, y_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1039, 4306) (1039,)\n",
      "Training set score: 0.960538979788258\n",
      "\n",
      "Test set score: 0.6753246753246753\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "train = lr.fit(X_train2, y_train2)\n",
    "print(X_train2.shape, y_train2.shape)\n",
    "print('Training set score:', lr.score(X_train2, y_train2))\n",
    "print('\\nTest set score:', lr.score(X_test2, y_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.8026948989412896\n",
      "\n",
      "Test set score: 0.6406926406926406\n"
     ]
    }
   ],
   "source": [
    "clf = GradientBoostingClassifier()\n",
    "train = clf.fit(X_train2, y_train2)\n",
    "\n",
    "print('Training set score:', clf.score(X_train2, y_train2))\n",
    "print('\\nTest set score:', clf.score(X_test2, y_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, we achieved a maximum testing accuracy score of 68.3%, which was accomplished using Logistic Regression on the word_count dataset created from the paragraphs compilation of the three Shakespearean works, without the additional features (nouns, verbs, adverbs, sentence length, etc) included. The highest level of variance represented by the set of components was also from the paragraph dataset, with approximately 50% of the total variance represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
